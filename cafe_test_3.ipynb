{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cafe_test_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VUwrPssAgZQvdSBFvAgd7Yb9uerkVV4Q",
      "authorship_tag": "ABX9TyM86txn/tF4VVLYDOA7EDP/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KRiver28/prac/blob/master/cafe_test_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_GDY4ZxUtkH",
        "outputId": "00a74433-a1c5-462d-e653-720ad526a94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "%cd '/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub-word 사전 읽어온다.\n",
        "with open('chatbot_cafe_test.pkl', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MAX_LEN = 15            # 단어 시퀀스 길이\n",
        "MODEL_PATH = '/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_cafetest_trained.h5'"
      ],
      "metadata": {
        "id": "pyiB0v0ZUzx-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 과정에서 생성한 SentencePiece model을 불러온다.\n",
        "SPM_MODEL = \"/content/drive/MyDrive/BIGDATA_STUDY/NLP/chatbot/chatbot_model.model\"\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(SPM_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWwe6-N5Uzs5",
        "outputId": "5be36c2e-c4d0-49a9-fac9-13101c2dff76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)"
      ],
      "metadata": {
        "id": "5wpZ-uFDUzlr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "# -------\n",
        "encoderX = Input(batch_shape=(None, MAX_LEN))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "_, eh2, ec2 = encLSTM2(ey1)         # LSTM 2층"
      ],
      "metadata": {
        "id": "uEGb4ehKU17P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "# -------\n",
        "# Decoder는 1개 단어씩을 입력으로 받는다.\n",
        "decoderX = Input(batch_shape=(None, 1))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(dy2)"
      ],
      "metadata": {
        "id": "74qpfu7lU14Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "# -----\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.load_weights(MODEL_PATH)"
      ],
      "metadata": {
        "id": "bVJ1mAFWU119"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatting용 model\n",
        "model_enc = Model(encoderX, [eh1, ec1, eh2, ec2])\n",
        "\n",
        "ih1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ih2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "\n",
        "dec_output1, dh1, dc1 = decLSTM1(decEMB, initial_state = [ih1, ic1])\n",
        "dec_output2, dh2, dc2 = decLSTM2(dec_output1, initial_state = [ih2, ic2])\n",
        "\n",
        "dec_output = decOutput(dec_output2)\n",
        "model_dec = Model([decoderX, ih1, ic1, ih2, ic2], [dec_output, dh1, dc1, dh2, dc2])"
      ],
      "metadata": {
        "id": "pqOXK3r_U1zr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question을 입력받아 Answer를 생성한다.\n",
        "def genAnswer(question):\n",
        "    question = question[np.newaxis, :]\n",
        "    init_h1, init_c1, init_h2, init_c2 = model_enc.predict(question)\n",
        "\n",
        "    # 시작 단어는 <BOS>로 한다.\n",
        "    word = np.array(sp.bos_id()).reshape(1, 1)\n",
        "\n",
        "    answer = []\n",
        "    for i in range(MAX_LEN):\n",
        "        dY, next_h1, next_c1, next_h2, next_c2 = model_dec.predict([word, init_h1, init_c1, init_h2, init_c2])\n",
        "        \n",
        "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
        "        # argmax로 해당 단어를 채택한다.\n",
        "        nextWord = np.argmax(dY[0, 0])\n",
        "\n",
        "        # 예상 단어가 <EOS>이거나 <PAD>이면 더 이상 예상할 게 없다.\n",
        "        if nextWord == sp.eos_id() or nextWord == sp.pad_id():\n",
        "            break\n",
        "        \n",
        "        # 다음 예상 단어인 디코더의 출력을 answer에 추가한다.\n",
        "        answer.append(idx2word[nextWord])\n",
        "        \n",
        "        # 디코더의 다음 recurrent를 위해 입력 데이터와 hidden 값을\n",
        "        # 준비한다. 입력은 word이고, hidden은 h와 c이다.\n",
        "        word = np.array(nextWord).reshape(1,1)\n",
        "    \n",
        "        init_h1 = next_h1\n",
        "        init_c1 = next_c1\n",
        "        init_h2 = next_h2\n",
        "        init_c2 = next_c2\n",
        "        \n",
        "    return sp.decode_pieces(answer)\n",
        "\n",
        "def make_question(que_string):\n",
        "    q_idx = []\n",
        "    for x in sp.encode_as_pieces(que_string):\n",
        "        if x in word2idx:\n",
        "            q_idx.append(word2idx[x])\n",
        "        else:\n",
        "            q_idx.append(sp.unk_id())   # out-of-vocabulary (OOV)\n",
        "    \n",
        "    # <PAD>를 삽입한다.\n",
        "    if len(q_idx) < MAX_LEN:\n",
        "        q_idx.extend([sp.pad_id()] * (MAX_LEN - len(q_idx)))\n",
        "    else:\n",
        "        q_idx = q_idx[0:MAX_LEN]\n",
        "    return q_idx"
      ],
      "metadata": {
        "id": "XeKxjZNdU1xn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatting\n",
        "# dummy : 최초 1회는 모델을 로드하는데 약간의 시간이 걸리므로 이것을 가리기 위함.\n",
        "def chatting(n=100):\n",
        "    for i in range(n):\n",
        "        question = input('Q : ')\n",
        "        \n",
        "        if  question == 'quit':\n",
        "            break\n",
        "        \n",
        "        q_idx = make_question(question)\n",
        "        answer = genAnswer(np.array(q_idx))\n",
        "        print('A :', answer)"
      ],
      "metadata": {
        "id": "htElW6S-U5u7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Chatting 시작 #######\n",
        "print(\"\\nSeq2Seq ChatBot (ver. 1.0)\")\n",
        "print(\"Chatting 모듈을 로드하고 있습니다 ...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR-P3mb8U5tE",
        "outputId": "34f07ea5-457a-49ba-c308-04293407d643"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Seq2Seq ChatBot (ver. 1.0)\n",
            "Chatting 모듈을 로드하고 있습니다 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 처음 1회는 시간이 걸리기 때문에 dummy question을 입력한다.\n",
        "answer = genAnswer(np.zeros(MAX_LEN))\n",
        "print(\"ChatBot이 준비 됐습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhqS364vU5q2",
        "outputId": "c2fa7aa8-e245-440f-9578-f9da91ad2447"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatBot이 준비 됐습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 채팅을 시작한다.\n",
        "chatting(100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6eADzpDU5ou",
        "outputId": "e51ba9e3-10b8-4fef-d77b-4b868ed42b93"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q : 팬케이크 아메리카노 세트 구성은 메인메뉴 두 개가 끝인가요?\n",
            "A : 네 메인메뉴 두가지로 구성됩니다\n",
            "Q : 테이크아웃으로 주문하면 캔 컵에 담아주시는 건가요?\n",
            "A : 아니요 따로 담아드립니다\n",
            "Q : 아이스크림 무제한 토핑 이벤트는 어떻게 참여하는 건가요?\n",
            "A : 무료 마카롱 1종세트는 구매 하나 변경되었습니다\n",
            "Q : 혹시 매장에서 사용가능한 와이파이는 비밀번호가 있을까요?\n",
            "A : 네 000lotte 입력하시면 바로 연결 되실 거예요\n",
            "Q : 제가 9일 날짜로 예약했는데요, 이름은 #이름# 이고요 주문 수량만 조금 변경할게요.\n",
            "A : 네 그 팬케이크로 다시 보내드릴게요\n",
            "Q : 마카롱은 매일 아침에 직접 제작 하고 계신가요?\n",
            "A : 네 매일 아침에 제조하고 있어서 내일 들어옵니다\n",
            "Q : 나눠먹는 큐브 와츄원은 몇 가지 맛이 들어가나요?\n",
            "A : 총 10가지 맛이 있습니다\n",
            "Q : 아이스크림 케이크에서 특정 맛을 빼거나 교체할 수 있을까요?\n",
            "A : 죄송하지만 쿠키 마카롱은 계절 딸기맛을 이용해 변경하실 수 있습니다\n",
            "Q : 플레인스콘에 들어가는 버터 브랜드가 무엇인가요?\n",
            "A : 네 저희  ⁇ 이 버터를 사용하고 있습니다\n",
            "Q : quit\n"
          ]
        }
      ]
    }
  ]
}