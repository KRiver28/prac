{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDl+u1BbkRqAg2aaoPcgJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KRiver28/prac/blob/master/chatbot_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hfnyIX-dJcI"
      },
      "outputs": [],
      "source": [
        "#https://github.com/haven-jeon/KoGPT2-chatbot/blob/master/train_torch.py\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Simsimi based on KoGPT-2')\n",
        "\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "\n",
        "parser.add_argument('--sentiment',\n",
        "                    type=str,\n",
        "                    default='0',\n",
        "                    help='sentiment for system. 0 is neutral, 1 is negative, 2 is positive.')\n",
        "\n",
        "parser.add_argument('--model_params',\n",
        "                    type=str,\n",
        "                    default='model_chp/model_-last.ckpt',\n",
        "                    help='model binary for starting chat')\n",
        "\n",
        "parser.add_argument('--train',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='for training')\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "U_TKN = '<usr>'\n",
        "S_TKN = '<sys>'\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'\n",
        "\n",
        "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
        "            pad_token=PAD, mask_token=MASK) \n",
        "\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, chats, max_len=32):\n",
        "        self._data = chats\n",
        "        self.first = True\n",
        "        self.q_token = U_TKN\n",
        "        self.a_token = S_TKN\n",
        "        self.sent_token = SENT\n",
        "        self.bos = BOS\n",
        "        self.eos = EOS\n",
        "        self.mask = MASK\n",
        "        self.pad = PAD\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = TOKENIZER \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        turn = self._data.iloc[idx]\n",
        "        q = turn['Q']\n",
        "        a = turn['A']\n",
        "        sentiment = str(turn['label'])\n",
        "        q_toked = self.tokenizer.tokenize(self.q_token + q + \\\n",
        "                                          self.sent_token + sentiment)   \n",
        "        q_len = len(q_toked)\n",
        "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
        "        a_len = len(a_toked)\n",
        "        if q_len + a_len > self.max_len:\n",
        "            a_len = self.max_len - q_len\n",
        "            if a_len <= 0:\n",
        "                q_toked = q_toked[-(int(self.max_len/2)):]\n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len\n",
        "                assert a_len > 0\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
        "        # [mask, mask, ...., mask, ..., <bos>,..A.. <eos>, <pad>....]\n",
        "        labels = [\n",
        "            self.mask,\n",
        "        ] * q_len + a_toked[1:]\n",
        "        if self.first:\n",
        "            logging.info(\"contexts : {}\".format(q))\n",
        "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
        "            logging.info(\"response : {}\".format(a))\n",
        "            logging.info(\"toked response : {}\".format(a_toked))\n",
        "            logging.info('labels {}'.format(labels))\n",
        "            self.first = False\n",
        "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
        "        self.max_len\n",
        "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
        "        while len(labels_ids) < self.max_len:\n",
        "            labels_ids += [self.tokenizer.pad_token_id]\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
        "        while len(token_ids) < self.max_len:\n",
        "            token_ids += [self.tokenizer.pad_token_id]\n",
        "        return(token_ids, np.array(mask),\n",
        "               labels_ids)\n",
        "\n",
        "\n",
        "class KoGPT2Chat(LightningModule):\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super(KoGPT2Chat, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.neg = -1e18\n",
        "        self.kogpt2 = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        # add model specific args\n",
        "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--max-len',\n",
        "                            type=int,\n",
        "                            default=32,\n",
        "                            help='max sentence length on input (default: 32)')\n",
        "\n",
        "        parser.add_argument('--batch-size',\n",
        "                            type=int,\n",
        "                            default=96,\n",
        "                            help='batch size for training (default: 96)')\n",
        "        parser.add_argument('--lr',\n",
        "                            type=float,\n",
        "                            default=5e-5,\n",
        "                            help='The initial learning rate')\n",
        "        parser.add_argument('--warmup_ratio',\n",
        "                            type=float,\n",
        "                            default=0.1,\n",
        "                            help='warmup ratio')\n",
        "        return parser\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (batch, seq_len, hiddens)\n",
        "        output = self.kogpt2(inputs, return_dict=True)\n",
        "        return output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        token_ids, mask, label = batch\n",
        "        out = self(token_ids)\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, self.neg * torch.ones_like(out))\n",
        "        loss = self.loss_function(mask_out.transpose(2, 1), label)\n",
        "        loss_avg = loss.sum() / mask.sum()\n",
        "        self.log('train_loss', loss_avg)\n",
        "        return loss_avg\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.lr, correct_bias=False)\n",
        "        # warm up lr\n",
        "        num_train_steps = len(self.train_dataloader()) * self.hparams.max_epochs\n",
        "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "        lr_scheduler = {'scheduler': scheduler, 'name': 'cosine_schedule_with_warmup',\n",
        "                        'monitor': 'loss', 'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _collate_fn(self, batch):\n",
        "        data = [item[0] for item in batch]\n",
        "        mask = [item[1] for item in batch]\n",
        "        label = [item[2] for item in batch]\n",
        "        return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        data = pd.read_csv('Chatbot_data/ChatbotData.csv')\n",
        "        self.train_set = CharDataset(data, max_len=self.hparams.max_len)\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_set, batch_size=self.hparams.batch_size, num_workers=2,\n",
        "            shuffle=True, collate_fn=self._collate_fn)\n",
        "        return train_dataloader\n",
        "\n",
        "    def chat(self, sent='0'):\n",
        "        tok = TOKENIZER\n",
        "        sent_tokens = tok.tokenize(sent)\n",
        "        with torch.no_grad():\n",
        "            while 1:\n",
        "                q = input('user > ').strip()\n",
        "                if q == 'quit':\n",
        "                    break\n",
        "                a = ''\n",
        "                while 1:\n",
        "                    input_ids = torch.LongTensor(tok.encode(U_TKN + q + SENT + sent + S_TKN + a)).unsqueeze(dim=0)\n",
        "                    pred = self(input_ids)\n",
        "                    gen = tok.convert_ids_to_tokens(\n",
        "                        torch.argmax(\n",
        "                            pred,\n",
        "                            dim=-1).squeeze().numpy().tolist())[-1]\n",
        "                    if gen == EOS:\n",
        "                        break\n",
        "                    a += gen.replace('â–', ' ')\n",
        "                print(\"Simsimi > {}\".format(a.strip()))\n",
        "\n",
        "\n",
        "parser = KoGPT2Chat.add_model_specific_args(parser)\n",
        "parser = Trainer.add_argparse_args(parser)\n",
        "args = parser.parse_args()\n",
        "logging.info(args)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if args.train:\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath='model_chp',\n",
        "            filename='{epoch:02d}-{train_loss:.2f}',\n",
        "            verbose=True,\n",
        "            save_last=True,\n",
        "            monitor='train_loss',\n",
        "            mode='min',\n",
        "            prefix='model_'\n",
        "        )\n",
        "        # python train_torch.py --train --gpus 1 --max_epochs 3\n",
        "        model = KoGPT2Chat(args)\n",
        "        model.train()\n",
        "        trainer = Trainer.from_argparse_args(\n",
        "            args,\n",
        "            checkpoint_callback=checkpoint_callback, gradient_clip_val=1.0)\n",
        "        trainer.fit(model)\n",
        "        logging.info('best model path {}'.format(checkpoint_callback.best_model_path))\n",
        "    if args.chat:\n",
        "        model = KoGPT2Chat.load_from_checkpoint(args.model_params)\n",
        "        model.chat()"
      ]
    }
  ]
}